{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b633c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flair\n",
      "  Downloading flair-0.11.3-py3-none-any.whl (401 kB)\n",
      "     ------------------------------------- 401.9/401.9 kB 12.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: torch!=1.8,>=1.5.0 in c:\\users\\4lab\\anaconda3\\envs\\p38\\lib\\site-packages (from flair) (1.10.2)\n",
      "Collecting scikit-learn>=0.21.3\n",
      "  Downloading scikit_learn-1.1.1-cp38-cp38-win_amd64.whl (7.3 MB)\n",
      "     ---------------------------------------- 7.3/7.3 MB 10.6 MB/s eta 0:00:00\n",
      "Collecting konoha<5.0.0,>=4.0.0\n",
      "  Downloading konoha-4.6.5-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: lxml in c:\\users\\4lab\\anaconda3\\envs\\p38\\lib\\site-packages (from flair) (4.8.0)\n",
      "Collecting more-itertools\n",
      "  Downloading more_itertools-8.13.0-py3-none-any.whl (51 kB)\n",
      "     ---------------------------------------- 51.6/51.6 kB 2.6 MB/s eta 0:00:00\n",
      "Collecting segtok>=1.5.7\n",
      "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: regex in c:\\users\\4lab\\anaconda3\\envs\\p38\\lib\\site-packages (from flair) (2022.4.24)\n",
      "Collecting ftfy\n",
      "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "     ---------------------------------------- 53.1/53.1 kB ? eta 0:00:00\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\4lab\\anaconda3\\envs\\p38\\lib\\site-packages (from flair) (0.6.0)\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in c:\\users\\4lab\\anaconda3\\envs\\p38\\lib\\site-packages (from flair) (3.3.4)\n",
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "     ------------------------------------- 981.5/981.5 kB 10.3 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting mpld3==0.3\n",
      "  Downloading mpld3-0.3.tar.gz (788 kB)\n",
      "     ------------------------------------- 788.5/788.5 kB 12.6 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\4lab\\anaconda3\\envs\\p38\\lib\\site-packages (from flair) (2.8.2)\n",
      "Requirement already satisfied: tqdm>=4.26.0 in c:\\users\\4lab\\anaconda3\\envs\\p38\\lib\\site-packages (from flair) (4.64.0)\n",
      "Collecting wikipedia-api\n",
      "  Downloading Wikipedia-API-0.5.4.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting sqlitedict>=1.6.0\n",
      "  Downloading sqlitedict-2.0.0.tar.gz (46 kB)\n",
      "     ---------------------------------------- 46.3/46.3 kB ? eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting tabulate\n",
      "  Downloading tabulate-0.8.10-py3-none-any.whl (29 kB)\n",
      "Collecting conllu>=4.0\n",
      "  Downloading conllu-4.4.2-py2.py3-none-any.whl (15 kB)\n",
      "Collecting hyperopt>=0.2.7\n",
      "  Downloading hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
      "     ---------------------------------------- 1.6/1.6 MB 11.2 MB/s eta 0:00:00\n",
      "Collecting deprecated>=1.2.4\n",
      "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting bpemb>=0.3.2\n",
      "  Downloading bpemb-0.3.3-py3-none-any.whl (19 kB)\n",
      "Collecting janome\n",
      "  Downloading Janome-0.4.2-py2.py3-none-any.whl (19.7 MB)\n",
      "     --------------------------------------- 19.7/19.7 MB 10.5 MB/s eta 0:00:00\n",
      "Collecting pptree\n",
      "  Downloading pptree-3.1.tar.gz (3.0 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting sentencepiece==0.1.95\n",
      "  Downloading sentencepiece-0.1.95-cp38-cp38-win_amd64.whl (1.2 MB)\n",
      "     ---------------------------------------- 1.2/1.2 MB 12.2 MB/s eta 0:00:00\n",
      "Collecting gensim>=3.4.0\n",
      "  Downloading gensim-4.2.0-cp38-cp38-win_amd64.whl (24.0 MB)\n",
      "     --------------------------------------- 24.0/24.0 MB 10.4 MB/s eta 0:00:00\n",
      "Collecting gdown==4.4.0\n",
      "  Downloading gdown-4.4.0.tar.gz (14 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: transformers>=4.0.0 in c:\\users\\4lab\\anaconda3\\envs\\p38\\lib\\site-packages (from flair) (4.19.2)\n",
      "Requirement already satisfied: six in c:\\users\\4lab\\anaconda3\\envs\\p38\\lib\\site-packages (from gdown==4.4.0->flair) (1.16.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\4lab\\anaconda3\\envs\\p38\\lib\\site-packages (from gdown==4.4.0->flair) (3.7.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\4lab\\anaconda3\\envs\\p38\\lib\\site-packages (from gdown==4.4.0->flair) (4.11.1)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\4lab\\anaconda3\\envs\\p38\\lib\\site-packages (from gdown==4.4.0->flair) (2.27.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\4lab\\anaconda3\\envs\\p38\\lib\\site-packages (from bpemb>=0.3.2->flair) (1.22.3)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\4lab\\anaconda3\\envs\\p38\\lib\\site-packages (from deprecated>=1.2.4->flair) (1.14.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\4lab\\anaconda3\\envs\\p38\\lib\\site-packages (from gensim>=3.4.0->flair) (1.8.0)\n",
      "Collecting Cython==0.29.28\n",
      "  Using cached Cython-0.29.28-py2.py3-none-any.whl (983 kB)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\4lab\\anaconda3\\envs\\p38\\lib\\site-packages (from gensim>=3.4.0->flair) (5.2.1)\n",
      "Collecting cloudpickle\n",
      "  Downloading cloudpickle-2.1.0-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: future in c:\\users\\4lab\\anaconda3\\envs\\p38\\lib\\site-packages (from hyperopt>=0.2.7->flair) (0.18.2)\n",
      "Collecting networkx>=2.2\n",
      "  Downloading networkx-2.8.4-py3-none-any.whl (2.0 MB)\n",
      "     ---------------------------------------- 2.0/2.0 MB 8.0 MB/s eta 0:00:00\n",
      "Collecting py4j\n",
      "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
      "     -------------------------------------- 199.7/199.7 kB 6.1 MB/s eta 0:00:00\n",
      "Collecting overrides<4.0.0,>=3.0.0\n",
      "  Downloading overrides-3.1.0.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting importlib-metadata<4.0.0,>=3.7.0\n",
      "  Downloading importlib_metadata-3.10.1-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\4lab\\anaconda3\\envs\\p38\\lib\\site-packages (from matplotlib>=2.2.3->flair) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\4lab\\anaconda3\\envs\\p38\\lib\\site-packages (from matplotlib>=2.2.3->flair) (9.0.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\4lab\\anaconda3\\envs\\p38\\lib\\site-packages (from matplotlib>=2.2.3->flair) (1.3.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\4lab\\anaconda3\\envs\\p38\\lib\\site-packages (from matplotlib>=2.2.3->flair) (3.0.4)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\4lab\\anaconda3\\envs\\p38\\lib\\site-packages (from scikit-learn>=0.21.3->flair) (1.1.0)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\4lab\\anaconda3\\envs\\p38\\lib\\site-packages (from torch!=1.8,>=1.5.0->flair) (4.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\4lab\\anaconda3\\envs\\p38\\lib\\site-packages (from tqdm>=4.26.0->flair) (0.4.4)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\4lab\\anaconda3\\envs\\p38\\lib\\site-packages (from transformers>=4.0.0->flair) (0.12.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\4lab\\anaconda3\\envs\\p38\\lib\\site-packages (from transformers>=4.0.0->flair) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\4lab\\anaconda3\\envs\\p38\\lib\\site-packages (from transformers>=4.0.0->flair) (6.0)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in c:\\users\\4lab\\anaconda3\\envs\\p38\\lib\\site-packages (from ftfy->flair) (0.2.5)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\4lab\\anaconda3\\envs\\p38\\lib\\site-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair) (3.7.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\4lab\\anaconda3\\envs\\p38\\lib\\site-packages (from requests[socks]->gdown==4.4.0->flair) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\4lab\\anaconda3\\envs\\p38\\lib\\site-packages (from requests[socks]->gdown==4.4.0->flair) (2022.5.18.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\4lab\\anaconda3\\envs\\p38\\lib\\site-packages (from requests[socks]->gdown==4.4.0->flair) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\4lab\\anaconda3\\envs\\p38\\lib\\site-packages (from requests[socks]->gdown==4.4.0->flair) (3.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\4lab\\anaconda3\\envs\\p38\\lib\\site-packages (from beautifulsoup4->gdown==4.4.0->flair) (2.3.1)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\4lab\\anaconda3\\envs\\p38\\lib\\site-packages (from requests[socks]->gdown==4.4.0->flair) (1.7.1)\n",
      "Building wheels for collected packages: gdown, mpld3, sqlitedict, langdetect, pptree, wikipedia-api, overrides\n",
      "  Building wheel for gdown (pyproject.toml): started\n",
      "  Building wheel for gdown (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for gdown: filename=gdown-4.4.0-py3-none-any.whl size=14759 sha256=cf7481bf7f069334c163c698a0644cd1c8ec3c76562a7f766e0e3efbeb87384f\n",
      "  Stored in directory: c:\\users\\4lab\\appdata\\local\\pip\\cache\\wheels\\7b\\7b\\5d\\656f46cd6889e4c93977be9586901d0adc1271b2d876c84c96\n",
      "  Building wheel for mpld3 (setup.py): started\n",
      "  Building wheel for mpld3 (setup.py): finished with status 'done'\n",
      "  Created wheel for mpld3: filename=mpld3-0.3-py3-none-any.whl size=116686 sha256=2f5717552015c06c3262cd1201ae7816ba6fa9f7aaa1a90ca20ba2fe549eddf6\n",
      "  Stored in directory: c:\\users\\4lab\\appdata\\local\\pip\\cache\\wheels\\3d\\9f\\9d\\d806a20bd97bc7076d724fa3e69fa5be61836ba16b2ffa6126\n",
      "  Building wheel for sqlitedict (setup.py): started\n",
      "  Building wheel for sqlitedict (setup.py): finished with status 'done'\n",
      "  Created wheel for sqlitedict: filename=sqlitedict-2.0.0-py3-none-any.whl size=15734 sha256=72421e8a83fd2b63587f5bf4d1c64bb7d6302295b727bf2b0527b21e1b5d52cf\n",
      "  Stored in directory: c:\\users\\4lab\\appdata\\local\\pip\\cache\\wheels\\ee\\0b\\8c\\3cdf3e7eef4161d79c62df5bef35b0614238d0d2bd3051877a\n",
      "  Building wheel for langdetect (setup.py): started\n",
      "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=16d43ee0df1c0c7beee9f968156ebfe76a2bdd18d18288c83871a7640f2d719b\n",
      "  Stored in directory: c:\\users\\4lab\\appdata\\local\\pip\\cache\\wheels\\13\\c7\\b0\\79f66658626032e78fc1a83103690ef6797d551cb22e56e734\n",
      "  Building wheel for pptree (setup.py): started\n",
      "  Building wheel for pptree (setup.py): finished with status 'done'\n",
      "  Created wheel for pptree: filename=pptree-3.1-py3-none-any.whl size=4609 sha256=d7db51c3dab00493486996cdadbdde03e14bb5c5da8fe24c7eb6267fda893591\n",
      "  Stored in directory: c:\\users\\4lab\\appdata\\local\\pip\\cache\\wheels\\e1\\8b\\30\\5b20240d3d13a9dfafb6a6dd49d1b541c86d39812cb3690edf\n",
      "  Building wheel for wikipedia-api (setup.py): started\n",
      "  Building wheel for wikipedia-api (setup.py): finished with status 'done'\n",
      "  Created wheel for wikipedia-api: filename=Wikipedia_API-0.5.4-py3-none-any.whl size=13476 sha256=2e0fe328f1d7656858e51a69c3d243411a3ec230efb47a392c6d67ee47d4b8e0\n",
      "  Stored in directory: c:\\users\\4lab\\appdata\\local\\pip\\cache\\wheels\\ed\\88\\e3\\da3d4d73cb91d659488cfa25913b84bbc26febec99d257bce9\n",
      "  Building wheel for overrides (setup.py): started\n",
      "  Building wheel for overrides (setup.py): finished with status 'done'\n",
      "  Created wheel for overrides: filename=overrides-3.1.0-py3-none-any.whl size=10172 sha256=9478f0d5e0efc6f2a882dbd24d6a91c6e29fce78039a75c2ad91e89ef71e2fa4\n",
      "  Stored in directory: c:\\users\\4lab\\appdata\\local\\pip\\cache\\wheels\\6a\\4f\\72\\28857f75625b263e2e3f5ab2fc4416c0a85960ac6485007eaa\n",
      "Successfully built gdown mpld3 sqlitedict langdetect pptree wikipedia-api overrides\n",
      "Installing collected packages: sqlitedict, sentencepiece, py4j, pptree, overrides, mpld3, janome, threadpoolctl, tabulate, segtok, networkx, more-itertools, langdetect, importlib-metadata, ftfy, deprecated, Cython, conllu, cloudpickle, wikipedia-api, scikit-learn, konoha, hyperopt, gensim, gdown, bpemb, flair\n",
      "  Attempting uninstall: sentencepiece\n",
      "    Found existing installation: sentencepiece 0.1.96\n",
      "    Uninstalling sentencepiece-0.1.96:\n",
      "      Successfully uninstalled sentencepiece-0.1.96\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 4.11.3\n",
      "    Uninstalling importlib-metadata-4.11.3:\n",
      "      Successfully uninstalled importlib-metadata-4.11.3\n",
      "Successfully installed Cython-0.29.28 bpemb-0.3.3 cloudpickle-2.1.0 conllu-4.4.2 deprecated-1.2.13 flair-0.11.3 ftfy-6.1.1 gdown-4.4.0 gensim-4.2.0 hyperopt-0.2.7 importlib-metadata-3.10.1 janome-0.4.2 konoha-4.6.5 langdetect-1.0.9 more-itertools-8.13.0 mpld3-0.3 networkx-2.8.4 overrides-3.1.0 pptree-3.1 py4j-0.10.9.5 scikit-learn-1.1.1 segtok-1.5.11 sentencepiece-0.1.95 sqlitedict-2.0.0 tabulate-0.8.10 threadpoolctl-3.1.0 wikipedia-api-0.5.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sphinx 5.0.0 requires importlib-metadata>=4.4; python_version < \"3.10\", but you have importlib-metadata 3.10.1 which is incompatible.\n",
      "markdown 3.3.7 requires importlib-metadata>=4.4; python_version < \"3.10\", but you have importlib-metadata 3.10.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install flair\n",
    "import logging\n",
    "from typing import Optional, List, Tuple, Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ee9c7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from presidio_analyzer import (\n",
    "    RecognizerResult,\n",
    "    EntityRecognizer,\n",
    "    AnalysisExplanation,\n",
    ")\n",
    "from presidio_analyzer.nlp_engine import NlpArtifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2ec658b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\4lab\\anaconda3\\envs\\p38\\lib\\site-packages\\pkg_resources\\__init__.py:123: PkgResourcesDeprecationWarning: 0.996-ko-0.9.2-msvc is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from flair.data import Sentence\n",
    "    from flair.models import SequenceTagger\n",
    "except ImportError:\n",
    "    print(\"Flair is not installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c569af15",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"presidio-analyzer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "852a12ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlairRecognizer(EntityRecognizer):\n",
    "    \"\"\"\n",
    "    Wrapper for a flair model, if needed to be used within Presidio Analyzer.\n",
    "\n",
    "    :example:\n",
    "    >from presidio_analyzer import AnalyzerEngine, RecognizerRegistry\n",
    "\n",
    "    >flair_recognizer = FlairRecognizer()\n",
    "\n",
    "    >registry = RecognizerRegistry()\n",
    "    >registry.add_recognizer(flair_recognizer)\n",
    "\n",
    "    >analyzer = AnalyzerEngine(registry=registry)\n",
    "\n",
    "    >results = analyzer.analyze(\n",
    "    >    \"My name is Christopher and I live in Irbid.\",\n",
    "    >    language=\"en\",\n",
    "    >    return_decision_process=True,\n",
    "    >)\n",
    "    >for result in results:\n",
    "    >    print(result)\n",
    "    >    print(result.analysis_explanation)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ENTITIES = [\n",
    "        \"LOCATION\",\n",
    "        \"PERSON\",\n",
    "        \"ORGANIZATION\",\n",
    "        # \"MISCELLANEOUS\"   # - There are no direct correlation with Presidio entities.\n",
    "    ]\n",
    "\n",
    "    DEFAULT_EXPLANATION = \"Identified as {} by Flair's Named Entity Recognition\"\n",
    "\n",
    "    CHECK_LABEL_GROUPS = [\n",
    "        ({\"LOCATION\"}, {\"LOC\", \"LOCATION\"}),\n",
    "        ({\"PERSON\"}, {\"PER\", \"PERSON\"}),\n",
    "        ({\"ORGANIZATION\"}, {\"ORG\"}),\n",
    "        # ({\"MISCELLANEOUS\"}, {\"MISC\"}), # Probably not PII\n",
    "    ]\n",
    "\n",
    "    MODEL_LANGUAGES = {\n",
    "        \"en\": \"flair/ner-english-large\",\n",
    "        \"es\": \"flair/ner-spanish-large\",\n",
    "        \"de\": \"flair/ner-german-large\",\n",
    "        \"nl\": \"flair/ner-dutch-large\",\n",
    "    }\n",
    "\n",
    "    PRESIDIO_EQUIVALENCES = {\n",
    "        \"PER\": \"PERSON\",\n",
    "        \"LOC\": \"LOCATION\",\n",
    "        \"ORG\": \"ORGANIZATION\",\n",
    "        # 'MISC': 'MISCELLANEOUS'   # - Probably not PII\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        supported_language: str = \"en\",\n",
    "        supported_entities: Optional[List[str]] = None,\n",
    "        check_label_groups: Optional[Tuple[Set, Set]] = None,\n",
    "        model: SequenceTagger = None,\n",
    "    ):\n",
    "        self.check_label_groups = (\n",
    "            check_label_groups if check_label_groups else self.CHECK_LABEL_GROUPS\n",
    "        )\n",
    "\n",
    "        supported_entities = supported_entities if supported_entities else self.ENTITIES\n",
    "        self.model = (\n",
    "            model\n",
    "            if model\n",
    "            else SequenceTagger.load(self.MODEL_LANGUAGES.get(supported_language))\n",
    "        )\n",
    "\n",
    "        super().__init__(\n",
    "            supported_entities=supported_entities,\n",
    "            supported_language=supported_language,\n",
    "            name=\"Flair Analytics\",\n",
    "        )\n",
    "\n",
    "    def load(self) -> None:\n",
    "        \"\"\"Load the model, not used. Model is loaded during initialization.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def get_supported_entities(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Return supported entities by this model.\n",
    "\n",
    "        :return: List of the supported entities.\n",
    "        \"\"\"\n",
    "        return self.supported_entities\n",
    "\n",
    "    # Class to use Flair with Presidio as an external recognizer.\n",
    "    def analyze(\n",
    "        self, text: str, entities: List[str], nlp_artifacts: NlpArtifacts = None\n",
    "    ) -> List[RecognizerResult]:\n",
    "        \"\"\"\n",
    "        Analyze text using Text Analytics.\n",
    "\n",
    "        :param text: The text for analysis.\n",
    "        :param entities: Not working properly for this recognizer.\n",
    "        :param nlp_artifacts: Not used by this recognizer.\n",
    "        :param language: Text language. Supported languages in MODEL_LANGUAGES\n",
    "        :return: The list of Presidio RecognizerResult constructed from the recognized\n",
    "            Flair detections.\n",
    "        \"\"\"\n",
    "\n",
    "        results = []\n",
    "\n",
    "        sentences = Sentence(text)\n",
    "        self.model.predict(sentences)\n",
    "\n",
    "        # If there are no specific list of entities, we will look for all of it.\n",
    "        if not entities:\n",
    "            entities = self.supported_entities\n",
    "\n",
    "        for entity in entities:\n",
    "            if entity not in self.supported_entities:\n",
    "                continue\n",
    "\n",
    "            for ent in sentences.get_spans(\"ner\"):\n",
    "                if not self.__check_label(\n",
    "                    entity, ent.labels[0].value, self.check_label_groups\n",
    "                ):\n",
    "                    continue\n",
    "                textual_explanation = self.DEFAULT_EXPLANATION.format(\n",
    "                    ent.labels[0].value\n",
    "                )\n",
    "                explanation = self.build_flair_explanation(\n",
    "                    round(ent.score, 2), textual_explanation\n",
    "                )\n",
    "                flair_result = self._convert_to_recognizer_result(ent, explanation)\n",
    "\n",
    "                results.append(flair_result)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _convert_to_recognizer_result(self, entity, explanation) -> RecognizerResult:\n",
    "\n",
    "        entity_type = self.PRESIDIO_EQUIVALENCES.get(entity.tag, entity.tag)\n",
    "        flair_score = round(entity.score, 2)\n",
    "\n",
    "        flair_results = RecognizerResult(\n",
    "            entity_type=entity_type,\n",
    "            start=entity.start_pos,\n",
    "            end=entity.end_pos,\n",
    "            score=flair_score,\n",
    "            analysis_explanation=explanation,\n",
    "        )\n",
    "\n",
    "        return flair_results\n",
    "\n",
    "    def build_flair_explanation(\n",
    "        self, original_score: float, explanation: str\n",
    "    ) -> AnalysisExplanation:\n",
    "        \"\"\"\n",
    "        Create explanation for why this result was detected.\n",
    "\n",
    "        :param original_score: Score given by this recognizer\n",
    "        :param explanation: Explanation string\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        explanation = AnalysisExplanation(\n",
    "            recognizer=self.__class__.__name__,\n",
    "            original_score=original_score,\n",
    "            textual_explanation=explanation,\n",
    "        )\n",
    "        return explanation\n",
    "\n",
    "    @staticmethod\n",
    "    def __check_label(\n",
    "        entity: str, label: str, check_label_groups: Tuple[Set, Set]\n",
    "    ) -> bool:\n",
    "        return any(\n",
    "            [entity in egrp and label in lgrp for egrp, lgrp in check_label_groups]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4468b3fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae7e489147a5490a992a9479d78005bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-24 18:23:40,679 loading file C:\\Users\\4lab\\.flair\\models\\ner-english-large\\07301f59bb8cb113803be316267f06ddf9243cdbba92a4c8067ef92442d2c574.554244d3476d97501a766a98078421817b14654496b86f2f7bd139dc502a4f29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1818c8bbb1949d1a207cbb54cd8c750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/616 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3038dc085b748e89ce7327e0ffa5933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/4.83M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a04cda6f075a4131b6b2425f9d17f336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/8.68M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-24 18:24:30,749 SequenceTagger predicts: Dictionary with 20 tags: <unk>, O, S-ORG, S-MISC, B-PER, E-PER, S-LOC, B-ORG, E-ORG, I-PER, S-PER, B-MISC, I-MISC, E-MISC, I-ORG, B-LOC, E-LOC, I-LOC, <START>, <STOP>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Span' object has no attribute 'start_pos'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m registry\u001b[38;5;241m.\u001b[39madd_recognizer(flair_recognizer)\n\u001b[0;32m     12\u001b[0m analyzer \u001b[38;5;241m=\u001b[39m AnalyzerEngine(registry\u001b[38;5;241m=\u001b[39mregistry)\n\u001b[1;32m---> 14\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43manalyzer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMy name is Christopher and I live in Irbid.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_decision_process\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(result)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\p38\\lib\\site-packages\\presidio_analyzer\\analyzer_engine.py:198\u001b[0m, in \u001b[0;36mAnalyzerEngine.analyze\u001b[1;34m(self, text, language, entities, correlation_id, score_threshold, return_decision_process, ad_hoc_recognizers, context)\u001b[0m\n\u001b[0;32m    195\u001b[0m     recognizer\u001b[38;5;241m.\u001b[39mis_loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# analyze using the current recognizer and append the results\u001b[39;00m\n\u001b[1;32m--> 198\u001b[0m current_results \u001b[38;5;241m=\u001b[39m \u001b[43mrecognizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentities\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mentities\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnlp_artifacts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnlp_artifacts\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current_results:\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;66;03m# add recognizer name to recognition metadata inside results\u001b[39;00m\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;66;03m# if not exists\u001b[39;00m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__add_recognizer_name_if_not_exists(current_results, recognizer)\n",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36mFlairRecognizer.analyze\u001b[1;34m(self, text, entities, nlp_artifacts)\u001b[0m\n\u001b[0;32m    126\u001b[0m         textual_explanation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mDEFAULT_EXPLANATION\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    127\u001b[0m             ent\u001b[38;5;241m.\u001b[39mlabels[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue\n\u001b[0;32m    128\u001b[0m         )\n\u001b[0;32m    129\u001b[0m         explanation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_flair_explanation(\n\u001b[0;32m    130\u001b[0m             \u001b[38;5;28mround\u001b[39m(ent\u001b[38;5;241m.\u001b[39mscore, \u001b[38;5;241m2\u001b[39m), textual_explanation\n\u001b[0;32m    131\u001b[0m         )\n\u001b[1;32m--> 132\u001b[0m         flair_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_to_recognizer_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43ment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexplanation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(flair_result)\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36mFlairRecognizer._convert_to_recognizer_result\u001b[1;34m(self, entity, explanation)\u001b[0m\n\u001b[0;32m    140\u001b[0m entity_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mPRESIDIO_EQUIVALENCES\u001b[38;5;241m.\u001b[39mget(entity\u001b[38;5;241m.\u001b[39mtag, entity\u001b[38;5;241m.\u001b[39mtag)\n\u001b[0;32m    141\u001b[0m flair_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(entity\u001b[38;5;241m.\u001b[39mscore, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    143\u001b[0m flair_results \u001b[38;5;241m=\u001b[39m RecognizerResult(\n\u001b[0;32m    144\u001b[0m     entity_type\u001b[38;5;241m=\u001b[39mentity_type,\n\u001b[1;32m--> 145\u001b[0m     start\u001b[38;5;241m=\u001b[39m\u001b[43mentity\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_pos\u001b[49m,\n\u001b[0;32m    146\u001b[0m     end\u001b[38;5;241m=\u001b[39mentity\u001b[38;5;241m.\u001b[39mend_pos,\n\u001b[0;32m    147\u001b[0m     score\u001b[38;5;241m=\u001b[39mflair_score,\n\u001b[0;32m    148\u001b[0m     analysis_explanation\u001b[38;5;241m=\u001b[39mexplanation,\n\u001b[0;32m    149\u001b[0m )\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m flair_results\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Span' object has no attribute 'start_pos'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    from presidio_analyzer import AnalyzerEngine, RecognizerRegistry\n",
    "\n",
    "    flair_recognizer = (\n",
    "        FlairRecognizer()\n",
    "    )  # This would download a very large (+2GB) model on the first run\n",
    "\n",
    "    registry = RecognizerRegistry()\n",
    "    registry.add_recognizer(flair_recognizer)\n",
    "\n",
    "    analyzer = AnalyzerEngine(registry=registry)\n",
    "\n",
    "    results = analyzer.analyze(\n",
    "        \"My name is Christopher and I live in Irbid.\",\n",
    "        language=\"en\",\n",
    "        return_decision_process=True,\n",
    "    )\n",
    "    for result in results:\n",
    "        print(result)\n",
    "        print(result.analysis_explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b225b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
